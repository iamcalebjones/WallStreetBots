{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will process text from the comments dataset \n",
    "\n",
    "Run LDA model\n",
    "\n",
    "Run KMeans Clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from collections import Counter\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guesswhatihate</td>\n",
       "      <td>#DIPPING</td>\n",
       "      <td>1607957482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k12nmonky</td>\n",
       "      <td>NIO and SPCE ftw</td>\n",
       "      <td>1607957482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tnmtnmtnm</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1607957482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everlastingdeath</td>\n",
       "      <td>Lol what happened to the limit up everyone's b...</td>\n",
       "      <td>1607957481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZakkuTurner</td>\n",
       "      <td>Jan 22 2021, 40.5</td>\n",
       "      <td>1607957481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               body  \\\n",
       "0    guesswhatihate                                           #DIPPING   \n",
       "1         k12nmonky                                   NIO and SPCE ftw   \n",
       "2         tnmtnmtnm                                               Flat   \n",
       "3  everlastingdeath  Lol what happened to the limit up everyone's b...   \n",
       "4       ZakkuTurner                                  Jan 22 2021, 40.5   \n",
       "\n",
       "   created_utc  \n",
       "0   1607957482  \n",
       "1   1607957482  \n",
       "2   1607957482  \n",
       "3   1607957481  \n",
       "4   1607957481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_comments = pd.read_csv('data/main_comments.csv')\n",
    "main_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7491333, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(main_comments)):\n",
    "    corpus.append([main_comments.body[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main functions for processing the text of each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several of the processing functions were copied from the NLP lecture\n",
    "\n",
    "\n",
    "# the toked function:\n",
    "# removes newline characters\n",
    "# removes apostrophes, easier to just remove apostrophes and leave naked contractions rather than expand them\n",
    "    # there's only like what a handful of common contractions, they can be words for this as far as i'm concerned\n",
    "# removes links that have the format 'https://etc'\n",
    "# removes special characters\n",
    "# replaces any number in the string with num\n",
    "# returns this split stripped and joined back together to remove multiple blanks\n",
    "# runs this through the nltk RegexTokenizer modified to handle emojis, which splits individual emojis\n",
    "    # so that three rocketships is 3 occurrences of one rocketship, etc. instead of its own separate character\n",
    "# finally translates emojis from their emoji symbol to a word representation of it so that ngrams captures these\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "link_regex = re.compile(r'(https://)[a-z./0-9?=&;-]*')\n",
    "numbers = re.compile(r'([0-9],?)+')\n",
    "apostrophe = re.compile(r\"\\'|’\") # heads up for both versions of the apostrophe!! dammit ' and ’\n",
    "newline = re.compile(r'(\\\\n)')\n",
    "specials = re.compile(r'[-+~`:;@=\\'’\"”#“$|,%/&^•·)(\\]\\[\\*\\\\?!\\._]')\n",
    "def toked(doc):\n",
    "    modified = newline.sub(' ', doc)\n",
    "    modified = apostrophe.sub('', modified)\n",
    "    modified = link_regex.sub('', modified)\n",
    "    modified = specials.sub('', modified)\n",
    "    modified = numbers.sub('', modified)\n",
    "    modified = ' '.join(modified.split()).strip()\n",
    "    modified = tokenizer.tokenize(modified)\n",
    "    \n",
    "    translated = []\n",
    "    for word in modified:\n",
    "        if word in emoji.UNICODE_EMOJI_ENGLISH:\n",
    "            translated.append('emoji_' + emoji.UNICODE_EMOJI_ENGLISH[word])\n",
    "        else:\n",
    "            translated.append(word)\n",
    "    \n",
    "    translated = ' '.join(translated)\n",
    "    modified = ' '.join(specials.sub('', translated).split()).strip()\n",
    "            \n",
    "    return modified\n",
    "\n",
    "\n",
    "# lemmatize\n",
    "def lemmatize(doc):\n",
    "    # get the words in the document\n",
    "    words = re.findall(\"\\w+|[^\\w\\s]\", doc)\n",
    "    # get the parts of speech\n",
    "    pos_tokens = nltk.pos_tag(words)\n",
    "    \n",
    "    # process_word(*token) uses star args to supply both word and part of speech to process_word\n",
    "    # for token in pos_tokens - we want to do this for every token in the document\n",
    "    \n",
    "    return ' '.join([process_word(*token) for token in pos_tokens])\n",
    "\n",
    "\n",
    "def process_word(word, pos):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_dict = {'J':'a', 'V':'v', 'N':'n', 'R': 'r'}\n",
    "    if pos[0] in pos_dict:\n",
    "        return lemmatizer.lemmatize(word.lower(), pos_dict[pos[0]])\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word.lower())\n",
    "    \n",
    "\n",
    "# extract n-grams\n",
    "def get_ngrams(doc):\n",
    "    # build vectorizer and analyzer\n",
    "    vectorizer = CountVectorizer(ngram_range=(2,3), preprocessor=None, tokenizer=None).build_analyzer()\n",
    "    # return the n-grams of size 2 and 3 without stop words in this case\n",
    "    return vectorizer(doc, stop_words=stopwords.words('english'))\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "my_additional_stop_words =  ('im', '', 'ud', 'pc', 'ampxb', 'unum', 'numc', 'unuma', 'unumd', 'num', 'numm', 'numk', 'xnumb', 'amp', 'im', 'ampxnumb', 'numlt', 'numnd', \"ampxnumb\")\n",
    "stop_words = text.ENGLISH_STOP_WORDS #.union(my_additional_stop_words)\n",
    "real_words = ''\n",
    "def remove_stop_words(words):\n",
    "#     words_split = words.split()\n",
    "#     for word in words.split\n",
    "    return [word for word in words.split() if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function for doing all the text processing at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full processing workflow, removing stop words before generating ngrams\n",
    "def full_processing(doc):\n",
    "    tokens = toked(str(doc)) # returns a string\n",
    "    words = lemmatize(tokens) # returns a string\n",
    "    words = remove_stop_words(words) # returns a list\n",
    "    ngrams = get_ngrams(' '.join(words)) # returns a list\n",
    "    return words + ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2500)\n",
      "0 ['buy', 'hold', 'money', 'sell', 'im', 'today', 'dip', 'good', 'lose', 'happen']\n",
      "1 ['gme', 'youre', 'yolo', 'flair', 'come', 'fund', 'question', 'bot', 'action', 'change']\n",
      "2 ['emojigemstone', 'delete', 'emojieggplant', 'emojiraisinghands', 'emojieggplant emojieggplant', 'emojieggplant emojieggplant emojieggplant', 'hold', 'emojigemstone emojiraisinghands', 'robinhood', 'emojigemstone emojigemstone']\n",
      "3 ['stock', 'like', 'u', 'look', 'just', 'market', 'think', 'people', 'company', 'big']\n",
      "4 ['emojirocket', 'emojirocket emojirocket', 'emojirocket emojirocket emojirocket', 'nok', 'emojigemstone', 'gme', 'love', 'man', 'emojigorilla', 'check']\n",
      "5 ['amc', 'retard', 'way', 'people', 'long', 'play', 'yes', 'yeah', 'im', 'new']\n",
      "6 ['share', 'buy', 'price', 'right', 'know', 'sell', 'just', 'week', 'dont', 'try']\n",
      "7 ['short', 'need', 'time', 'lol', 'day', 'squeeze', 'gonna', 'thats', 'trade', 'trading']\n",
      "8 ['use', 'bb', 'post', 'tsla', 'tesla', 'best', '⣿', 'comment', 'dude', 'crash']\n",
      "9 ['fuck', 'make', 'shit', 'let', 'want', 'dont', 'year', 'guy', 'account', 'say']\n",
      "CPU times: user 4h 10min 1s, sys: 1h 43min 11s, total: 5h 53min 13s\n",
      "Wall time: 6h 7min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# the processed doc \n",
    "processed = []\n",
    "for doc in corpus:\n",
    "    processed.append(full_processing(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None, \n",
    "                             max_df=0.7, \n",
    "                             min_df=2, \n",
    "                             max_features=2500,\n",
    "                             tokenizer=None,\n",
    "                             lowercase=False, \n",
    "                             preprocessor=None,\n",
    "                             analyzer=lambda x: x)\n",
    "#                              ngram_range=(1,3))\n",
    "\n",
    "X = vectorizer.fit_transform(processed)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42, learning_method='online', n_jobs=-1)\n",
    "lda.fit(X)\n",
    "print(lda.components_.shape)\n",
    "top_components = lda.components_.argsort()[:,-1:-11:-1]\n",
    "\n",
    "# top_components.shape\n",
    "for i, v in enumerate(top_components):\n",
    "    print(i, [features[i] for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
